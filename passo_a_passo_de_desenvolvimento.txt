1o - Primeiras configurações de ambiente e add template base do Django em estrutura para docker-compose

python3 -m venv venv [PASTA VAI SER IGNORADA PELO GIT]

pip freeze

pip install pip --upgrade

pip install django

pip freeze [3 items]


sudo apt install python3-django

mkdir djangoapp [Vai ser a raiz do projeto django]

cd djangoapp 

django-admin startproject project . [VAI ADD O TAMPLATE BASE DO DJANGO]

cd ..

python manage.py [vai ser]-> python3 djangoapp/manage.py runserver


-------------------------------------------------------------------------------------------------------------------------------

2o - Add dependencias inicias do python para o docker e remove boilerplate do dbsqlite.


rm -r ./djangoapp/project/db.sqlite

cd ./djangoapp

Cria o requirements.txt com:
	Django>=4.2.1,<4.3
	psycopg2-binary>=2.9.6,<2.10 [DRIVER DO POSTGRE]


-------------------------------------------------------------------------------------------------------------------------------

3o - variáveis de ambiente dotenv-files

mkdir dotenv-files && cd ./dotenv-files

Cria o .env com as constantes que vão ser utilizadas no settings.py do Django.

Cria a secretkey [NÃO VOU UTILIZAR]

cd djangoapp/project 

Ajustes no settings.py puxando as variáveis constantes do .env (JÁ PENSANDO NA ESTRUTURA DE DIR PARA DOCKER)

Ajustes no urls.py deixando linear com o settings.py (JÁ PENSANDO NA ESTRUTURA DE DIR PARA DOCKER)


-------------------------------------------------------------------------------------------------------------------------------

4o - configurações do dockerfile e scripts bash para ser executado no processo de montagem da imagem no container


-------------------------------------------------------------------------------------------------------------------------------

5o - docker-compose 

Cria o docker-compose.yml com as configurações para ambos os Dockerfiles

primeira vez: sudo docker-compose up --build

VAI DAR ERROS POR CONTA DA CONSTANTE POSTGRES_HOST DEFINIDA NO `.env` NO DIR dotenv-files
	- Isso ocorre pois o host no docker não é localhost e sim o nome do container.
	
docker stop all: sudo docker stop $(sudo docker ps -a -q) ; sudo docker system prune -f ; sudo docker rm -vf $(sudo docker ps -aq) ;

docker remove all: sudo docker stop $(sudo docker ps -a -q) ; sudo docker system prune -f ; sudo docker rm -vf $(sudo docker ps -aq) ; sudo docker rmi -f $(sudo docker images -aq) ;


docker-compose up --build --force-recreate 


OBS: TODA VEZ QUE ALTERAR OS ARQUIVOS DE CONFIGURAÇÃO DO DOCKER EM GERAL, DEVE-SE REMOVER CACHE E RE-BUILDAR COM
docker-compose up --build --force-recreate

	- APENAS QUANDO FOR ARQUIVOS DOCKER, PARA ARQUIVOS EM GERAL DE DESENVOLVIMENTO ELE JÁ REFLETE AS MUDANÇAS COPIANDO ELAS PARA O CONTAINER.
	
	

ERRO DE PERMISSÃO AO BUILDAR, MESMO EXECUTANDO COM SUDO:




	djangoapp    | Traceback (most recent call last):
djangoapp    |   File "/djangoapp/manage.py", line 22, in <module>
djangoapp    |     main()
djangoapp    |   File "/djangoapp/manage.py", line 18, in main
djangoapp    |     execute_from_command_line(sys.argv)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/__init__.py", line 442, in execute_from_command_line
djangoapp    |     utility.execute()
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/__init__.py", line 436, in execute
djangoapp    |     self.fetch_command(subcommand).run_from_argv(self.argv)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/base.py", line 412, in run_from_argv
djangoapp    |     self.execute(*args, **cmd_options)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/base.py", line 458, in execute
djangoapp    |     output = self.handle(*args, **options)
djangoapp    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py", line 209, in handle
djangoapp    |     collected = self.collect()
djangoapp    |                 ^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py", line 135, in collect
djangoapp    |     handler(path, prefixed_path, storage)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py", line 378, in copy_file
djangoapp    |     self.storage.save(prefixed_path, source_file)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/files/storage/base.py", line 38, in save
djangoapp    |     name = self._save(name, content)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/files/storage/filesystem.py", line 87, in _save
djangoapp    |     os.makedirs(directory, exist_ok=True)
djangoapp    |   File "<frozen os>", line 215, in makedirs
djangoapp    |   File "<frozen os>", line 225, in makedirs
djangoapp    | PermissionError: [Errno 13] Permission denied: '/data/web/static/admin'
djangoapp exited with code 1
	



DEBUGANDO PARA VERIFICAR AS PERMISSÕES:

Step 10/13 : RUN ls -la /data/web &&     chown -R duser:duser /data/web/static &&     chmod -R 755 /data/web/static &&     ls -la /data/web
 ---> Running in 0093d4e95f83
total 16
drwxr-xr-x    4 root     root          4096 Jan 10 19:44 .
drwxr-xr-x    3 root     root          4096 Jan 10 19:44 ..
drwxr-xr-x    2 duser    duser         4096 Jan 10 19:44 media
drwxr-xr-x    2 duser    duser         4096 Jan 10 19:44 static
total 24
drwxr-xr-x    1 root     root          4096 Jan 10 19:44 .
drwxr-xr-x    1 root     root          4096 Jan 10 19:44 ..
drwxr-xr-x    2 duser    duser         4096 Jan 10 19:44 media
drwxr-xr-x    1 duser    duser         4096 Jan 10 19:44 static



SOLUÇÃO: Add flag `--dry-run` no arquivo `./scripts/commands.sh`, ficando da seguinte forma:

	python manage.py collectstatic --dry-run --noinput
	

docker stop $(sudo docker ps -a -q) ; sudo docker system prune -f ; sudo docker rm -vf $(sudo docker ps -aq) ;

sudo docker stop $(sudo docker ps -a -q) ; sudo docker system prune -f ; sudo docker rm -vf $(sudo docker ps -aq) ; sudo docker rmi -f $(sudo docker images -aq) ;

sudo rm -r ./data

docker-compose up --build --force-recreate 

OUTPUT AGORA:

djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/core.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/autocomplete.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/inlines.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/prepopulate_init.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/jquery.init.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/filters.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/prepopulate.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/theme.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/nav_sidebar.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/change_form.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/urlify.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/calendar.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/collapse.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/SelectBox.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/cancel.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/actions.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/SelectFilter2.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/popup_response.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/select2.full.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/select2.full.min.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/LICENSE.md'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/fa.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/it.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/sl.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ar.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ka.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/pl.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/tr.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/hu.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ca.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/sq.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/bg.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/km.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/et.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/de.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/fr.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/tk.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ja.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/cs.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/da.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/hsb.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/sr.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/hi.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/mk.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/zh-CN.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/sr-Cyrl.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/uk.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/id.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/pt-BR.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/el.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/bn.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/en.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/he.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/nl.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/af.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/is.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ru.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ms.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ps.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/az.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ro.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ne.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/nb.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/hy.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/zh-TW.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/dsb.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/bs.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/vi.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/sk.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/fi.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/es.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/th.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/lt.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/eu.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/ko.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/pt.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/lv.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/hr.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/sv.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/select2/i18n/gl.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/xregexp/xregexp.min.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/xregexp/LICENSE.txt'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/xregexp/xregexp.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/jquery/LICENSE.txt'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/jquery/jquery.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/vendor/jquery/jquery.min.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/admin/RelatedObjectLookups.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/js/admin/DateTimeShortcuts.js'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/selector-icons.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-alert.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-clock.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-changelink.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-calendar.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/tooltag-add.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/README.txt'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/calendar-icons.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-no.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/tooltag-arrowright.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-unknown-alt.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/search.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-deletelink.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-yes.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-viewlink.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-unknown.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/inline-delete.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/LICENSE'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/sorting-icons.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/icon-addlink.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/gis/move_vertex_off.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/img/gis/move_vertex_on.svg'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/dashboard.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/rtl.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/dark_mode.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/widgets.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/nav_sidebar.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/login.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/autocomplete.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/responsive_rtl.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/base.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/responsive.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/forms.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/changelists.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/vendor/select2/LICENSE-SELECT2.md'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/vendor/select2/select2.css'
djangoapp    | Pretending to copy '/venv/lib/python3.11/site-packages/django/contrib/admin/static/admin/css/vendor/select2/select2.min.css'
djangoapp    | 
djangoapp    | 125 static files copied to '/data/web/static'.
djangoapp    | No changes detected




-------------------------------------------------------------------------------------------------------------------------------

6o - Primeira rota com retorno view (HELLO WORLD)


docker-compose run djangoapp python manage.py startapp hello_world


VAI CRIAR O TEMPLATE BASE PARA A CRIAÇÃO DE ROTAS:


mkdir ./djangoapp/hello_world/templates/hello_world

cd ./djangoapp/hello_world/templates/hello_world

Cria o index.html desta rota com boilerplate `!`


cd ./djangoapp/hello_world/

Editar `views.py` definindo a função que retorna o ./djangoapp/hello_world/templates/hello_world/index.html

Copiar o `./djangoapp/project/urls.py

Remove boilerplates e deixa clean.

Editar `./djangoapp/project/urls.py` e add a rota do novo template hello_world

http://127.0.0.1:8000/

ERROR POIS FALTA REGISTRAR ESSE TEMPLATE NO `./djangoapp/project/seetings.py` com informações geradas no `./djangoapp/hello_world/apps.py`: 

	TemplateDoesNotExist at /



DA SEGUINTE FORMA: `./djangoapp/project/seetings.py`:

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'django.contrib.staticfiles',
    'hello_world',                      <---- AQUIII
]


CASO TIVESSE ALGUM MODEL, para mapear essa entidade e refletir no banco (Hibernate):

docker-compose run djangoapp python manage.py makemigrations

docker-compose run djangoapp python manage.py migrate

docker-compose run djangoapp python manage.py collectstatic


OBS: TODA VEZ QUE EXECUTAR O docker-compose up JÁ SERÁ EXECUTADO TUDO ISSO, POIS ESTA DEFINIDO EM `./scripts/commands.sh`


-------------------------------------------------------------------------------------------------------------------------------

7o - Primeira rota com retorno Json (HELLO WORLD_API) e Serialização das entidades


docker-compose run djangoapp python manage.py startapp hello_world_api 


Ajusta os boirlerplates para corresponder a uma Rest API com retornos Json


ERROR SEM MUITA INFORMAÇÃO UTIL PARA TRACKING, MAS RESOLVI:


djangoapp    | Internal Server Error: /api/jsonarrayofdou/
djangoapp    | Traceback (most recent call last):
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/handlers/exception.py", line 55, in inner
djangoapp    |     response = get_response(request)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/handlers/base.py", line 220, in _get_response
djangoapp    |     response = response.render()
djangoapp    |                ^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/template/response.py", line 114, in render
djangoapp    |     self.content = self.rendered_content
djangoapp    |                    ^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/rest_framework/response.py", line 70, in rendered_content
djangoapp    |     ret = renderer.render(self.data, accepted_media_type, context)
djangoapp    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/rest_framework/renderers.py", line 722, in render
djangoapp    |     template = loader.get_template(self.template)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/template/loader.py", line 19, in get_template
djangoapp    |     raise TemplateDoesNotExist(template_name, chain=chain)
djangoapp    | django.template.exceptions.TemplateDoesNotExist: rest_framework/api.html
djangoapp    | [10/Jan/2024 20:52:27] "GET /api/jsonarrayofdou/ HTTP/1.1" 500 74030
djangoapp    | Not Found: /api/j
djangoapp    | [10/Jan/2024 20:52:35] "GET /api/j HTTP/1.1" 404 3641
djangoapp    | Internal Server Error: /api/
djangoapp    | Traceback (most recent call last):
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/handlers/exception.py", line 55, in inner
djangoapp    |     response = get_response(request)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/handlers/base.py", line 220, in _get_response
djangoapp    |     response = response.render()
djangoapp    |                ^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/template/response.py", line 114, in render
djangoapp    |     self.content = self.rendered_content
djangoapp    |                    ^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/rest_framework/response.py", line 70, in rendered_content
djangoapp    |     ret = renderer.render(self.data, accepted_media_type, context)
djangoapp    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/rest_framework/renderers.py", line 722, in render
djangoapp    |     template = loader.get_template(self.template)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/template/loader.py", line 19, in get_template
djangoapp    |     raise TemplateDoesNotExist(template_name, chain=chain)
djangoapp    | django.template.exceptions.TemplateDoesNotExist: rest_framework/api.html
djangoapp    | [10/Jan/2024 20:52:37] "GET /api/ HTTP/1.1" 500 73956
djangoapp    | Not Found: /favicon.ico
djangoapp    | [10/Jan/2024 20:52:38] "GET /favicon.ico HTTP/1.1" 404 2502


SOLUÇÃO: Add `rest_framework` em `./djangoapp/project/settings` (Mesmo não sendo o nome do projeto "hello_world_api" deve colocar essa flag)


	INSTALLED_APPS = [
	    'django.contrib.admin',
	    'django.contrib.auth',
	    'django.contrib.contenttypes',
	    'django.contrib.sessions',
	    'django.contrib.messages',
	    'django.contrib.staticfiles',
	    'hello_world',
	    'hello_world_api',
	    'rest_framework',
	]



-------------------------------------------------------------------------------------------------------------------------------

8 - Criando a rest API separada apenas para os gatilhos no scraping e alimentação deles no db.


ENDPOINTS Á DESENVOLVER:
	
	Varre tudo da home do https://www.in.gov.br/leiturajornal [SEM SALVAR EM BANCO, Para salvar &saveInDB=true]
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/ 
	
	
	
	Varre os DOU da seção mencionada na data atual [SEM SALVAR EM BANCO, Para salvar &saveInDB=true]
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`dou1 | dou2 | dou3`
	
	
	
	Varre os DOU da seção mencionada e na data mencionada [SEM SALVAR EM BANCO, Para salvar &saveInDB=true]
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`dou1 | dou2 | dou3`&data=`DD-MM-AAAA`
	
	

docker-compose run djangoapp python manage.py startapp trigger_web_scraping_dou_api


Cria o primeiro Endpoint que faz a raspagem (PORÉM AINDA NÃO SALVA EM BANCO)

	Varre tudo da home do https://www.in.gov.br/leiturajornal [SEM SALVAR EM BANCO, Para salvar &saveInDB=true]
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/ 


- OBS: Retorno sem paginação por enquanto.


Retornando apenas o item JsonArray do json <script id='params'>, pois o json completo é muito poluído e dificil de entender quais membros são relevantes, como a ideia é após a análise concatenação item a item com `https://www.in.gov.br/en/web/dou/-/` + `jsonArray['urlTitle']`, então estou manipulando apenas o objeto interno jsonArray{...}   


-------------------------------------------------------------------------------------------------------------------------------

9 - Criando endpoint GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`do1 | do2 | do3`

Varre os DOU da seção mencionada no query string param, na data atual

- OBS: Retorno sem paginação por enquanto.


-------------------------------------------------------------------------------------------------------------------------------

10o - ScraperAPI Refactoring no views.py: Extraí lógicas de validações e cria classe especializada URLQueryStringParameterValidator no pacote validators.py


-------------------------------------------------------------------------------------------------------------------------------

11o - ScraperAPI: Add endpoint GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=`
- Obs1: Ainda não salvando em banco, vou implementar uma flag `?saveInDBFlag=true` depois.

- Obs2: Retorno sem paginação por enquanto.

- Já fazendo algumas validações no validator especializado.


-------------------------------------------------------------------------------------------------------------------------------

12o - ScraperAPI: Add logs em arquivo para os validators, não é boarat pica ficar expondo detalhes de erros para o mundo externo quando não existe autenticação e autorização.


-------------------------------------------------------------------------------------------------------------------------------

13o - ScraperAPI: Add endpoint GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1|do2|do3&data=DD-MM-AAAA`
- Obs1: Ainda não salvando em banco, vou implementar uma flag `?saveInDBFlag=true` depois.

- Obs2: Retorno sem paginação por enquanto.

- Já fazendo algumas validações no validator especializado, e logger nos erros.


-------------------------------------------------------------------------------------------------------------------------------

14o - ScraperAPI: Comaça a implementar a `?saveInDBFlag=true` para cada um dos enpoints:

	Varre tudo da home do https://www.in.gov.br/leiturajornal         <--------- PARA ESTE PRIMEIRO
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=true 
	
	
	
	Varre os DOU da seção mencionada na data atual
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`dou1 | dou2 | dou3`?saveInDBFlag=true
	
	
	Varre os DOU da data mencionada                             
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=`DD-MM-AAAA`?saveInDBFlag=true
	
	
	
	Varre os DOU da seção mencionada e na data mencionada
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`dou1 | dou2 | dou3`&data=`DD-MM-AAAA`?saveInDBFlag=true


Modificações em geral nas funções para não fazer duplicatas.

Adicionando a variável bool `saveInDBFlagURLQueryString` do parâmetro URL query string `?saveInDBFlag=true` para o handler genérico - GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/ que faz a requisição crua sem parametros. Ou seja, getAll




REQUISIÇÃO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=true 

Varre tudo da home do https://www.in.gov.br/leiturajornal e salva no banco.


-------------------------------------------------------------------------------------------------------------------------------

14o - Database DOU API: Após utilizar a API especializada para os gatilhos na raspagem, resolvi criar outra ROTA especializada em fazer as requisições para o banco local, ou seja, não implementar na mesma API especializada em raspagem para maior coesão.


- Essa ROTA `db_dou_api` faz requisições para a mesma tabela alimentada pelos triggers da `trigger_web_scraping_dou_api`

- Ou seja, utilizamos uma API para fazer os disparos das raspagens e se quiser alimentamos o banco.

- E Posteriormente, podemos acessar via outra ROTA esses dados já armazenados no banco local.

- OBS: Ainda não faz verificações para evitar duplicatas de registros!! Ou seja, toda vez que executar o gatilho GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=true vai appender os mesmos registros no insert into..


- SOLUÇÃO QUE VOU IMPLEMENTAR PARA EVITAR DUPLICATAS: Utilizar o atributo "urlTitle": "resolucao-re-n-95-de-10-de-janeiro-de-2024-537019492" como chave primária. 


- MELHOR SOLUÇÃO VISANDO PERFORMANCE: Antes de fazer a raspagem e salvar, verificar se já possue registros no banco local da data mencionada, e verificar se a quantidade de registros é a mesma no portal do DOU, desta forma, só iriamos inserir novamente a diferença entre os dois conjuntos.


-------------------------------------------------------------------------------------------------------------------------------

15o - Altera a chave primária da entidade JournalJsonArrayOfDOU utilizando o attr `urlTitle` pois ele é único, desta forma não vão ocorrer duplicatas (DISTINCT) de registros quando disparado os triggers de raspagem várias vezes.

	[models.JournalJsonArrayOfDOU]:
	
	urlTitle = models.CharField(max_length=255, primary_key=True, unique=True)


docker exec -it psql psql -U CHANGE-ME -d db_poder360

DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;

quit

docker-compose run djangoapp python manage.py makemigrations

docker-compose run djangoapp python manage.py migrate

docker exec -it psql psql -U CHANGE-ME -d db_poder360

\d trigger_web_scraping_dou_api_journaljsonarrayofdou;


OUTPUT:

	db_poder360=# \d trigger_web_scraping_dou_api_journaljsonarrayofdou;
	      Table "public.trigger_web_scraping_dou_api_journaljsonarrayofdou"
	       Column       |          Type          | Collation | Nullable | Default 
	--------------------+------------------------+-----------+----------+---------
	 pubName            | character varying(255) |           | not null | 
	 urlTitle           | character varying(255) |           | not null | 
	 numberPage         | integer                |           | not null | 
	 subTitulo          | text                   |           | not null | 
	 titulo             | text                   |           | not null | 
	 title              | text                   |           | not null | 
	 pubDate            | date                   |           | not null | 
	 content            | text                   |           | not null | 
	 editionNumber      | integer                |           | not null | 
	 hierarchyLevelSize | integer                |           | not null | 
	 artType            | character varying(255) |           | not null | 
	 pubOrder           | character varying(255) |           | not null | 
	 hierarchyStr       | text                   |           | not null | 
	 hierarchyList      | jsonb                  |           | not null | 
	Indexes:
	    "trigger_web_scraping_dou__urlTitle_b95cfbb1_pk" PRIMARY KEY, btree ("urlTitle")
	    "trigger_web_scraping_dou_urlTitle_b95cfbb1_like" btree ("urlTitle" varchar_pattern_ops)
    

EXECUTA O TRIGGER: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=1


APAGA O ULTIMO REGISTRO

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'resolucao-re-n-95-de-10-de-janeiro-de-2024-537019492';
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   162
	(1 row)
	
AGORA EXECUTA O TRIGGER DENOVO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=1

	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   163
	(1 row)

PRONTO, SÓ ESTÁ SALVANDO A DIFERENÇA ENTRE OS DOIS CONJUNTOS.


-------------------------------------------------------------------------------------------------------------------------------
    
16o - Como foi alterado a chave primária do JournalJsonArrayOfDOU para `urlTitle`, refletiu problemas em outras partes do código: Isso ocorreu pois o django rest framework utiliza o atributo id para algumas tarefas de controle interno, então a solução é fazer rollback da tarefa anterior e alterar a lógica que garante não duplicar os elementos (fazendo comparações equals com o campo `urlTitle` antes de insert into)


ANTES: FUNCIONAL PARA GERAR ID AUTO INCREMENT pois o django não tem essa funcionalidade para campos que NÃO SEJAM A CHAVE PRIMÁRIA ( id = models.AutoField(primary_key=True) ) <--- SÓ FUNCIONA PARA CHAVE PRIMÁRIA.


ESSA FOI A SOLUÇÃO ANTERIOR PARA GERAR OS ID's DE MANEIRA INCREMENTAL AUTOMÁTICAMENTE EM CASOS EM QUE A CHAVE PRIMÁRIA NÃO PODE SER A COLUNA ID:

        global_last_id = JournalJsonArrayOfDOU.objects.all().aggregate(largest=models.Max('id'))['largest']
        
        if global_last_id is None:
            global_last_id = 1
        
        dou_journals_toBe_insert = []

        for obj in dou_journals_jsonArrayField_list:
            journal_obj = JournalJsonArrayOfDOU(
                id=global_last_id,
                urlTitle=obj.urlTitle,
                pubName=obj.pubName,
                numberPage=obj.numberPage,
                subTitulo=obj.subTitulo,
                titulo=obj.titulo,
                title=obj.title,
                pubDate=obj.pubDate,
                content=obj.content,
                editionNumber=obj.editionNumber,
                hierarchyLevelSize=obj.hierarchyLevelSize,
                artType=obj.artType,
                pubOrder=obj.pubOrder,
                hierarchyStr=obj.hierarchyStr,
                hierarchyList=obj.hierarchyList
            )
            dou_journals_toBe_insert.append(journal_obj)

            global_last_id += 1
            
            

SOLUÇÃO AGORA: ROLLBACK PARA A TAREFA 15, PORÉM COM PEQUENAS MUDANÇAS NA LÓGICA QUE GARANTE DISTINCT DOS RECORDS.


	[... CODIGOS ...]


REFLETINDO AS ALTERAÇÕES NO BANCO COM MIGRATIONS: 

	docker-compose run djangoapp python manage.py makemigrations

	docker-compose run djangoapp python manage.py migrate


ERROR COM EXPLICAÇÕES DELE ABAIXO E COMO SOLUCIONAR TAMBÉM:


psql         | 2024-01-12 16:43:10.069 UTC [61] ERROR:  multiple primary keys for table "trigger_web_scraping_dou_api_journaljsonarrayofdou" are not allowed
psql         | 2024-01-12 16:43:10.069 UTC [61] STATEMENT:  ALTER TABLE "trigger_web_scraping_dou_api_journaljsonarrayofdou" ADD CONSTRAINT "trigger_web_scraping_dou__id_cb3d5ddd_pk" PRIMARY KEY ("id")
djangoapp    |   Applying trigger_web_scraping_dou_api.0004_alter_journaljsonarrayofdou_id_and_more...Traceback (most recent call last):
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/utils.py", line 87, in _execute
djangoapp    |     return self.cursor.execute(sql)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    | psycopg2.errors.InvalidTableDefinition: multiple primary keys for table "trigger_web_scraping_dou_api_journaljsonarrayofdou" are not allowed
djangoapp    | 
djangoapp    | 
djangoapp    | The above exception was the direct cause of the following exception:
djangoapp    | 
djangoapp    | Traceback (most recent call last):
djangoapp    |   File "/djangoapp/manage.py", line 22, in <module>
djangoapp    |     main()
djangoapp    |   File "/djangoapp/manage.py", line 18, in main
djangoapp    |     execute_from_command_line(sys.argv)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/__init__.py", line 442, in execute_from_command_line
djangoapp    |     utility.execute()
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/__init__.py", line 436, in execute
djangoapp    |     self.fetch_command(subcommand).run_from_argv(self.argv)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/base.py", line 412, in run_from_argv
djangoapp    |     self.execute(*args, **cmd_options)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/base.py", line 458, in execute
djangoapp    |     output = self.handle(*args, **options)
djangoapp    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/base.py", line 106, in wrapper
djangoapp    |     res = handle_func(*args, **kwargs)
djangoapp    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/management/commands/migrate.py", line 356, in handle
djangoapp    |     post_migrate_state = executor.migrate(
djangoapp    |                          ^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/migrations/executor.py", line 135, in migrate
djangoapp    |     state = self._migrate_all_forwards(
djangoapp    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/migrations/executor.py", line 167, in _migrate_all_forwards
djangoapp    |     state = self.apply_migration(
djangoapp    |             ^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/migrations/executor.py", line 252, in apply_migration
djangoapp    |     state = migration.apply(state, schema_editor)
djangoapp    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/migrations/migration.py", line 132, in apply
djangoapp    |     operation.database_forwards(
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/migrations/operations/fields.py", line 235, in database_forwards
djangoapp    |     schema_editor.alter_field(from_model, from_field, to_field)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/base/schema.py", line 831, in alter_field
djangoapp    |     self._alter_field(
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/postgresql/schema.py", line 288, in _alter_field
djangoapp    |     super()._alter_field(
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/base/schema.py", line 1118, in _alter_field
djangoapp    |     self.execute(self._create_primary_key_sql(model, new_field))
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/postgresql/schema.py", line 48, in execute
djangoapp    |     return super().execute(sql, None)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/base/schema.py", line 201, in execute
djangoapp    |     cursor.execute(sql, params)
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/utils.py", line 102, in execute
djangoapp    |     return super().execute(sql, params)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/utils.py", line 67, in execute
djangoapp    |     return self._execute_with_wrappers(
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/utils.py", line 80, in _execute_with_wrappers
djangoapp    |     return executor(sql, params, many, context)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/utils.py", line 84, in _execute
djangoapp    |     with self.db.wrap_database_errors:
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/utils.py", line 91, in __exit__
djangoapp    |     raise dj_exc_value.with_traceback(traceback) from exc_value
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/db/backends/utils.py", line 87, in _execute
djangoapp    |     return self.cursor.execute(sql)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    | django.db.utils.ProgrammingError: multiple primary keys for table "trigger_web_scraping_dou_api_journaljsonarrayofdou" are not allowed



Isso ocorre quando estavamos tentando alterar a coluna da chave primária pelo migrations, ao executar:

	docker-compose run djangoapp python manage.py makemigrations

Será gerado o arquivo: 


	from django.db import migrations, models


	class Migration(migrations.Migration):

	    dependencies = [
		('trigger_web_scraping_dou_api', '0003_journaljsonarrayofdou_id'),
	    ]

	    operations = [
    		migrations.AlterField(
		    model_name='journaljsonarrayofdou',
		    name='id',                                                   <---- NOVA CHAVE PRIMÁRIA DEPOIS DE MIGRAR
		    field=models.AutoField(primary_key=True, serialize=False),
		),
		 migrations.AlterField(
		    model_name='journaljsonarrayofdou',
		    name='urlTitle',
		    field=models.CharField(max_length=255, unique=True),          <---- CHAVE PRIMÁRIA ATUAL ANTES DE MIGRAR
		),
	    ]


AO QUE ESTÁ O PROBLEMA: A ordem das duas migrações gerada automáticamente é relevante! 

Ao tentar aplicar as alterações para o primeiro:

		migrations.AlterField(
		    model_name='journaljsonarrayofdou',
		    name='id',                                                   <---- NOVA CHAVE PRIMÁRIA DEPOIS DE MIGRAR
		    field=models.AutoField(primary_key=True, serialize=False),
		),

Ocorre o erro, pois atualmente o campo `name='urlTitle'` é a chave primária.


SOLUÇÃO: Basta alterar a ordem para:



	from django.db import migrations, models


	class Migration(migrations.Migration):

	    dependencies = [
		('trigger_web_scraping_dou_api', '0003_journaljsonarrayofdou_id'),
	    ]

	    operations = [
    		migrations.AlterField(
		    model_name='journaljsonarrayofdou',
		    name='urlTitle',
		    field=models.CharField(max_length=255, unique=True),         <---- CHAVE PRIMÁRIA ATUAL ANTES DE MIGRAR
		),
    		migrations.AlterField(
		    model_name='journaljsonarrayofdou',
		    name='id',                                                   <---- NOVA CHAVE PRIMÁRIA DEPOIS DE MIGRAR
		    field=models.AutoField(primary_key=True, serialize=False),
		),
	    ]


Desta forma primeiro ele vai remover a chave primária do name='urlTitle' e depois aplicar no id.


TESTANDO: 

	EXECUTA O TRIGGER: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=1
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   239
	(1 row)


APAGA O ULTIMO REGISTRO

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'retificacao-537039836';
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   238
	(1 row)

	
AGORA EXECUTA O TRIGGER DENOVO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=1

	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   239
	(1 row)


PRONTO, SÓ ESTÁ SALVANDO A DIFERENÇA ENTRE OS DOIS CONJUNTOS.


-------------------------------------------------------------------------------------------------------------------------------

17o - ScraperAPI: Replicando a implementação `?saveInDBFlag=true` para cada um dos enpoints:

	Varre tudo da home do https://www.in.gov.br/leiturajornal         <--------- PRONTO NA TAREFA ANTERIOR
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=true
	
	
	
	Varre os DOU da seção mencionada na data atual                    <--------- PARA ESTE AGORA
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`dou1 | dou2 | dou3`?saveInDBFlag=true
	
	
	Varre os DOU da data mencionada                            
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=`DD-MM-AAAA`?saveInDBFlag=true
	
	
	
	Varre os DOU da seção mencionada e na data mencionada
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`dou1 | dou2 | dou3`&data=`DD-MM-AAAA`?saveInDBFlag=true


Modificações em geral nas funções para não fazer duplicatas, pois a parte maior deste desenvolvimento foi feito na tarefa do primeiro endpoint, agora é só ir replicando nas assinaturas de outros endpoints o parâmetro `saveInDBFlagURLQueryString : bool`.

Já vai refletir a funcionalidade de salvar para todos, pois todos chamam a mesma função genérica que recebe a URL e faz a requisição, e nela passamos essa flag para salvar ou não. (Já garantindo DISTINCT).


TESTANDO DOU1: 

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;

	EXECUTA O TRIGGER: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&saveInDBFlag=true
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   239
	(1 row)
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/              <------ CONSULTA O BANCO
	
		[... "count": 239, ...]
	
	http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2               <------ CONSULTA SEM SALVAR NO BANCO
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/    <------ CONSULTA O BANCO DENOVO, NÃO DEVE ALTERAR
	
		[... "count": 239, ...]
		
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;     <------ MESMA COISA QUE O DE CIMA
	 count 
	-------
	   239
	(1 row)
	

APAGA O ULTIMO REGISTRO

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'retificacao-537039836';
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   238
	(1 row)

	
AGORA EXECUTA O TRIGGER DENOVO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&saveInDBFlag=true

	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   239
	(1 row)
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'retificacao-537039836';                   <------ PARA GARANTIR, DEVE DELETAR MESMO ELEMENTO DE ANTES SEM ERROR
	
	
	
TESTANDO DOU2: 

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;

	EXECUTA O TRIGGER: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2&saveInDBFlag=true
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   792
	(1 row)
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/              <------ CONSULTA O BANCO
	
		[... "count": 792, ...]
	
	http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2               <------ CONSULTA SEM SALVAR NO BANCO
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/    <------ CONSULTA O BANCO DENOVO, NÃO DEVE ALTERAR
	
		[... "count": 792, ...]
		
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;     <------ MESMA COISA QUE O DE CIMA
	 count 
	-------
	   792
	(1 row)
	

APAGA O ULTIMO REGISTRO

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'portaria-de-pessoal-gm/ms-n-30-de-10-de-janeiro-de-2024-537091380';
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   791
	(1 row)

	
AGORA EXECUTA O TRIGGER DENOVO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2&saveInDBFlag=true

	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   792
	(1 row)
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'portaria-de-pessoal-gm/ms-n-30-de-10-de-janeiro-de-2024-537091380';                   <------ PARA GARANTIR, DEVE DELETAR MESMO ELEMENTO DE ANTES SEM ERROR
	

TESTANDO DOU3: 

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;

	EXECUTA O TRIGGER: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3&saveInDBFlag=true
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   2392
	(1 row)
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/              <------ CONSULTA O BANCO
	
		[... "count": 2392, ...]
	
	http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3               <------ CONSULTA SEM SALVAR NO BANCO
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/    <------ CONSULTA O BANCO DENOVO, NÃO DEVE ALTERAR
	
		[... "count": 2392, ...]
		
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;     <------ MESMA COISA QUE O DE CIMA
	 count 
	-------
	   2392
	(1 row)
	

APAGA O ULTIMO REGISTRO

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'edital-n-1-anvisa-de-11-de-janeiro-de-2024-537292877';
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   2391
	(1 row)

	
AGORA EXECUTA O TRIGGER DENOVO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3&saveInDBFlag=true

	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   2392
	(1 row)
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'edital-n-1-anvisa-de-11-de-janeiro-de-2024-537292877';                   <------ PARA GARANTIR, DEVE DELETAR MESMO ELEMENTO DE ANTES SEM ERROR


PRONTO, SÓ ESTÁ SALVANDO A DIFERENÇA ENTRE OS DOIS CONJUNTOS.



Refactoring ScraperAPI Service: Reduz a quantidade de loopings no insert into e obtém o mesmo resultado porém mais rapidamente. 

TESTANDO DOU1: 

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;

	EXECUTA O TRIGGER: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&saveInDBFlag=true
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   239
	(1 row)
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/              <------ CONSULTA O BANCO
	
		[... "count": 239, ...]
	
	http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2               <------ CONSULTA SEM SALVAR NO BANCO
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/    <------ CONSULTA O BANCO DENOVO, NÃO DEVE ALTERAR
	
		[... "count": 239, ...]
		
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;     <------ MESMA COISA QUE O DE CIMA
	 count 
	-------
	   239
	(1 row)
	

APAGA O ULTIMO REGISTRO

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'retificacao-537039836';
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   238
	(1 row)

	
AGORA EXECUTA O TRIGGER DENOVO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&saveInDBFlag=true

	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   239
	(1 row)
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'retificacao-537039836';                   <------ PARA GARANTIR, DEVE DELETAR MESMO ELEMENTO DE ANTES SEM ERROR
	
	
	
TESTANDO DOU2: 

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;

	EXECUTA O TRIGGER: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2&saveInDBFlag=true
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   792
	(1 row)
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/              <------ CONSULTA O BANCO
	
		[... "count": 792, ...]
	
	http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2               <------ CONSULTA SEM SALVAR NO BANCO
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/    <------ CONSULTA O BANCO DENOVO, NÃO DEVE ALTERAR
	
		[... "count": 792, ...]
		
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;     <------ MESMA COISA QUE O DE CIMA
	 count 
	-------
	   792
	(1 row)
	

APAGA O ULTIMO REGISTRO

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'portaria-de-pessoal-gm/ms-n-30-de-10-de-janeiro-de-2024-537091380';
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   791
	(1 row)

	
AGORA EXECUTA O TRIGGER DENOVO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&saveInDBFlag=true

	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   792
	(1 row)
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'portaria-de-pessoal-gm/ms-n-30-de-10-de-janeiro-de-2024-537091380';                   <------ PARA GARANTIR, DEVE DELETAR MESMO ELEMENTO DE ANTES SEM ERROR
	

TESTANDO DOU3: 

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;

	EXECUTA O TRIGGER: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3&saveInDBFlag=true
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   2392
	(1 row)
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/              <------ CONSULTA O BANCO
	
		[... "count": 2392, ...]
	
	http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3               <------ CONSULTA SEM SALVAR NO BANCO
	
	http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/    <------ CONSULTA O BANCO DENOVO, NÃO DEVE ALTERAR
	
		[... "count": 2392, ...]
		
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;     <------ MESMA COISA QUE O DE CIMA
	 count 
	-------
	   2392
	(1 row)
	

APAGA O ULTIMO REGISTRO

	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'edital-n-1-anvisa-de-11-de-janeiro-de-2024-537292877';
	
	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   2391
	(1 row)

	
AGORA EXECUTA O TRIGGER DENOVO: GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3&saveInDBFlag=true

	SELECT COUNT(*) FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	 count 
	-------
	   2392
	(1 row)
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'edital-n-1-anvisa-de-11-de-janeiro-de-2024-537292877';                   <------ PARA GARANTIR, DEVE DELETAR MESMO ELEMENTO DE ANTES SEM ERROR


PRONTO, SÓ ESTÁ SALVANDO A DIFERENÇA ENTRE OS DOIS CONJUNTOS.


-------------------------------------------------------------------------------------------------------------------------------
 
 			ESSE TEM UMA OBS IMPORTANTE!
 
18o - FINALIZA ScraperAPI com as informações gerais para posteriormente implementar os endpoints que detalham cada página single): 

Replicando a implementação `?saveInDBFlag=true` para cada um dos enpoints:

	Varre tudo da home do https://www.in.gov.br/leiturajornal         <--------- PRONTO
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=true 
	
	
	
	Varre os DOU da seção mencionada na data atual                    <--------- PRONTO NA TAREFA ANTERIOR
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`dou1 | dou2 | dou3`?saveInDBFlag=true
	
	
	
	Varre os DOU da data mencionada                                   <--------- PARA ESTE AGORA E O DEBAIXO
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=`DD-MM-AAAA`?saveInDBFlag=true
	
	
	
	Varre os DOU da seção mencionada e na data mencionada             <--------- PARA ESTE AGORA E O DE CIMA
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=`dou1 | dou2 | dou3`&data=`DD-MM-AAAA`?saveInDBFlag=true



Nesta tarefa eu implementei logo para os dois endpoints acima pois o padrão do portal https://www.in.gov.br/leiturajornal passando APENAS A DATA, é retornar os registros da seção DOU1, mas a ideia é retornar TODOS OS DOUS da data mencionada.


Então eu fiz as funções que recebe SEÇÃO e DATA, junto com esse endpoint da DATA sozinha, pois eu utilizo as funções especializadas dos dois parametros neste endpoint, a diferença é que eu passo a seção DOU1 e DOU2 e DOU3 na mão, mesmo que não explicitado na URL query string da chamada (Por conta deste problema no portal DOU do gov).


Eu reaproveito a função:

	run_scraper_with_all_params(url_param: str, secaoURLQueryString_param, dataURLQueryString_param, saveInDBFlagURLQueryString : bool):
		
Na chamada para os endpoints:

	GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=`DD-MM-AAAA`?saveInDBFlag=true
	
	GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=`DD-MM-AAAA`
	
	
REALIZO TAMBÉM TODOS OS MESMOS TESTES ANTERIORES, E MAIS ALGUNS PARA A ULTIMA ROTA DO JSONARRAY GENÉRICO:

	http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=11-01-2024&secao=do2&saveInDBFlag=true
	
	
	
	
OBS IMPORTANTE: AS VEZES OCORRE O SEGUINTE ERRO NO PORTAL DOU DO GOV:

ConnectionError at /trigger_web_scraping_dou_api/

HTTPSConnectionPool(host='www.in.gov.br', port=443): Max retries exceeded with url: /leiturajornal?data=11-01-2024&secao=do2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3fc80b4390>: Failed to establish a new connection: [Errno -3] Try again'))

Request Method: 	GET
Request URL: 	http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=11-01-2024&secao=do2&saveInDBFlag=true
Django Version: 	4.2.9
Exception Type: 	ConnectionError
Exception Value: 	

HTTPSConnectionPool(host='www.in.gov.br', port=443): Max retries exceeded with url: /leiturajornal?data=11-01-2024&secao=do2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3fc80b4390>: Failed to establish a new connection: [Errno -3] Try again'))

Exception Location: 	/venv/lib/python3.11/site-packages/requests/adapters.py, line 519, in send
Raised during: 	trigger_web_scraping_dou_api.views.ScraperViewSet
Python Executable: 	/venv/bin/python
Python Version: 	3.11.3
Python Path: 	

['/djangoapp',
 '/usr/local/lib/python311.zip',
 '/usr/local/lib/python3.11',
 '/usr/local/lib/python3.11/lib-dynload',
 '/venv/lib/python3.11/site-packages']

Server time: 	Fri, 12 Jan 2024 20:32:00 -0300

djangoapp    | 
djangoapp    | During handling of the above exception, another exception occurred:
djangoapp    | 
djangoapp    | Traceback (most recent call last):
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/handlers/exception.py", line 55, in inner
djangoapp    |     response = get_response(request)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/core/handlers/base.py", line 197, in _get_response
djangoapp    |     response = wrapped_callback(request, *callback_args, **callback_kwargs)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/views/decorators/csrf.py", line 56, in wrapper_view
djangoapp    |     return view_func(*args, **kwargs)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/django/views/generic/base.py", line 104, in view
djangoapp    |     return self.dispatch(request, *args, **kwargs)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/rest_framework/views.py", line 509, in dispatch
djangoapp    |     response = self.handle_exception(exc)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/rest_framework/views.py", line 469, in handle_exception
djangoapp    |     self.raise_uncaught_exception(exc)
djangoapp    |   File "/venv/lib/python3.11/site-packages/rest_framework/views.py", line 480, in raise_uncaught_exception
djangoapp    |     raise exc
djangoapp    |   File "/venv/lib/python3.11/site-packages/rest_framework/views.py", line 506, in dispatch
djangoapp    |     response = handler(request, *args, **kwargs)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/djangoapp/trigger_web_scraping_dou_api/views.py", line 60, in get
djangoapp    |     return Response(self.handle_all_params(secaoURLQueryString, dataURLQueryString, saveInDBFlagURLQueryString=True))
djangoapp    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/djangoapp/trigger_web_scraping_dou_api/views.py", line 97, in handle_all_params
djangoapp    |     return ScraperUtil.run_scraper_with_all_params(DOU_BASE_URL, secaoURLQueryString, dataURLQueryString, saveInDBFlagURLQueryString)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/djangoapp/trigger_web_scraping_dou_api/scrapers.py", line 103, in run_scraper_with_all_params
djangoapp    |     return ScraperUtil.run_generic_scraper(url_param, saveInDBFlagURLQueryString)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/djangoapp/trigger_web_scraping_dou_api/scrapers.py", line 17, in run_generic_scraper
djangoapp    |     response = scraper.get(url_param)
djangoapp    |                ^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/requests/sessions.py", line 602, in get
djangoapp    |     return self.request("GET", url, **kwargs)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/cfscrape/__init__.py", line 121, in request
djangoapp    |     resp = super(CloudflareScraper, self).request(method, url, *args, **kwargs)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/requests/sessions.py", line 589, in request
djangoapp    |     resp = self.send(prep, **send_kwargs)
djangoapp    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/requests/sessions.py", line 703, in send
djangoapp    |     r = adapter.send(request, **kwargs)
djangoapp    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp    |   File "/venv/lib/python3.11/site-packages/requests/adapters.py", line 519, in send
djangoapp    |     raise ConnectionError(e, request=request)
djangoapp    | requests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.in.gov.br', port=443): Max retries exceeded with url: /leiturajornal?data=11-01-2024&secao=do2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3fc80b4390>: Failed to establish a new connection: [Errno -3] Try again'))
djangoapp    | [12/Jan/2024 20:32:00] "GET /trigger_web_scraping_dou_api/?data=11-01-2024&secao=do2&saveInDBFlag=true HTTP/1.1" 500 191336


-------------------------------------------------------------------------------------------------------------------------------

19o - Começa endpoint para mais detalhamento de cada registro DOU com 'https://www.in.gov.br/en/web/dou/-/' + `urlTitle`

ConnectionError at /trigger_web_scraping_dou_api/

HTTPSConnectionPool(host='www.in.gov.br', port=443): Max retries exceeded with url: /en/web/dou/-//portaria-mds-n-952-de-29-de-dezembro-de-2023-535301404 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7eff19139e90>: Failed to establish a new connection: [Errno -5] Name has no usable address'))
	
	
-------------------------------------------------------------------------------------------------------------------------------	

20o - Finaliza endpoint para mais detalhamento de cada registro DOU com 'https://www.in.gov.br/en/web/dou/-/' + `urlTitle`

- Obs: Utilizando como exemplo o próprio registro demonstrativo enviado no E-mail do desafio.

	https://www.in.gov.br/en/web/dou/-/acordao-cofen-n-103-de-27-de-setembro-de-2022-459835961	

	Versão certificada: https://pesquisa.in.gov.br/imprensa/jsp/visualiza/index.jsp?data=25/01/2023&jornal=515&pagina=62

Endpoint:
	
	Busca detalhes de um registro e retorna no padrão solicitado no E-mail: (SEM SALVAR EM BANCO)
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailSingleDOUJournalWithUrlTitleField=acordao-cofen-n-103-de-27-de-setembro-de-2022-459835961
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'lei-n-14.802-de-10-de-janeiro-de-2024-anexo-537017145';
	
	
-------------------------------------------------------------------------------------------------------------------------------

21o - Otimização da performance com uso de paralelismo em pontos críticos:
	
	- GET ALL: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True
	
	- ANTES o get all de todos os dous do dia com detalhamento de cada registro (jornal) demorava 6 minutos (executando em fluxo sincrono). 
	
	- AGORA OTIMIZADO demora menos da metade.


- Obs: Durante os testes, eu tive que mockar a data para quinta feira "11-01-2024", pois final de semana não teve registros de nenhum jornal. MAS retirei o mock na hora do commit, o get all ou outras rotas sem passar parâmetro de data consideram o dia atual.	

-------------------------------------------------------------------------------------------------------------------------------

22o - Otimização MASTER (3x Mais rápido) da performance com uso de +3 Instâncias clones da API, paralelismo e async em pontos críticos:

- OBS: `.env` sofreu mudanças por conta disto! então removi ele do gitignore então não precisa configurar.


Após várias configurações visando aumentar a performance para o endpoint mais bruto:

	- GET ALL: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True


Percebi que apenas o paralelismo e async não seriam o suficiente, e a melhoria veio junto com a lição:


	- Para operações brutas de IO (network) a melhor técnica é async nas chamadas para requisições em massa.
	
	- Além disto, clonei + 3 instâncias da API, aonde cada uma é responsável pela lista de URL's de cada seção.
		
		
		
- Métricas ANTES de delegar a tarefa para mais instâncias da mesma API em paralelo e utilizar async:

	- GET ALL: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True
	
	
	
				MÉTRICAS (ANTES):
				
				
				
		Tempo de CPU do usuário 	204471.176 ms
		Tempo de CPU do sistema 	124701.111 ms
		Tempo total de CPU 			329172.287 ms
		Tempo decorrido 			319575.105 ms
		Mudanças de contexto 		369385 voluntário, 103707 involuntário
	


- Métricas utilizando + 3 instâncias clones da API e delegando:

		- GET ALL: http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True
	
	
	
				EXECUTA POR DEBAIXO DOS PANOS:
				
				
	
		- do1 para: http://127.0.0.1:8002/trigger_web_scraping_dou_api/?secao=do1
		- do2 para: http://127.0.0.1:8003/trigger_web_scraping_dou_api/?secao=do2
		- do3 para: http://127.0.0.1:8003/trigger_web_scraping_dou_api/?secao=do3
	
	
	
				MÉTRICAS (DEPOIS, OTIMIZADO):
	 
	 
	 

		Tempo de CPU do usuário 	7939.230 ms
		Tempo de CPU do sistema 	1653.996 ms
		Tempo total de CPU 			9593.226 ms
		Tempo decorrido 			101200.783 ms
		Mudanças de contexto 		4505 voluntário, 1523 involuntário
		
		
		
				RESUMO GERAL:



					   ANTES       DEPOIS
		Tempo de CPU do usuário: 204471.176 - 7939.230   = 196531.946 ms MAIS RÁPIDO!
		Tempo de CPU do sistema: 124701.111 - 1653.996   = 123047.115 ms MAIS RÁPIDO!
		Tempo total de CPU:	  	 329172.287 - 9593.226   = 319579.061 ms MAIS RÁPIDO!
		Tempo decorrido:		 319575.105 - 101200.783 = 218374.322 ms MAIS RÁPIDO!		
		
				
				
				CONHECIMENTO ADQUIRIDO:
			
			
			
	- concurrent.futures.ThreadPoolExecutor(): Essa abordagem é mais recomendada em operações de cálculos intensivos (o que não é o nosso caso de uso).
	
	- Assincronismo (async): Essa é a melhor abordagem para operações intensivas de IO (o que é o nosso caso), e foi a melhor de todas, reduzindo o tempo de espera drásticamente, além de executar mais instâncias da API ao mesmo tempo.
	
	- Utilizamos chamadas assíncronas com a lib `aiocsfcrape` que extends funcionalidades da `aiohttp`, ela une funcionalidades de async para IO em conjunto com o bypass para o Cloudflare.
		

-------------------------------------------------------------------------------------------------------------------------------

 23o - ScraperAPI: [Teste de performance] Disseminando otimização da performance ENDPOINT DA API: 
 
 VARRENDO COM NENHUM PARAMETRO, GETALL NA HOME: 
 
 Utilizando programação async (Porém não delegando para API's clones):

            Varre tudo da home do portal DOU gov porém NÃO SALVA em banco de dados:
            - GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/

		METRICAS ANTES: Utilizando paralelismo com ThreadPoolExecutor():

		# Executa a função passando como parâmetro cada elemento da lista individualmente, para cada thread diferente:
		# Porisso o `*len(dous_list)` nos outros parâmetros subsequentes que não é a "lista principal" de `dou_list`
		# Por conta deste comportamento aonde a função é executada para cada elemento da lista em paralelo.
		# Logo, os parâmetros devem ser listas com o mesmo tamanho, e como não são listas mesmo igual ao `dou_list`
		# então transformamos esses parâmetros em listas e utilizamos `len()` para que todos os parâmetros da função correspondam com o mesmo tamanho linearmente entre sí.

		```
		    dous_list = ['do1', 'do2', 'do3']
		    all_dous_with_current_date_dontDetails = []
		    with ThreadPoolExecutor() as executor:

		        all_dous_with_current_date_dontDetails = list(executor.map(ScraperUtil.run_scraper_with_all_params, dous_list, [date_now_db_and_brazilian_format]*len(dous_list), [saveInDBFlagURLQueryString]*len(dous_list)))

		    return all_dous_with_current_date_dontDetails ```

		Tempo de CPU do usuário 	4922.568 ms
		Tempo de CPU do sistema 	428.632 ms
		Tempo total de CPU 			5351.200 ms
		Tempo decorrido 			10498.939 ms
		Mudanças de contexto 		4152 voluntário, 188 involuntário

		METRICAS DEPOIS: Utilizando programação async (Porém não delegando para API's clones): RESULTADO PIOR QUE O ANTERIOR:

		# Aplica a mesma lógica das API's Clones, porém aqui não ganhamos performance!
		# Isso ocorre pois com poucas requisições executadas async não compensa devido aos processamentos adicionais relacionados ao assincronismo que ocorrem "por debaixo dos panos".
		# Processamentos esses que são de verificações e tudo mais que está implementado no async e nós não vemos pois o python abstraí essa complexidade.

		# Já nas API's Clones essa abordagem é mais eficiente, pois o processamento adicional implementado no async se torna irrelevante quando a massa de dados é maior.

		Tempo de CPU do usuário 	5252.706 ms
		Tempo de CPU do sistema 	395.352 ms
		Tempo total de CPU 			5648.058 ms
		Tempo decorrido 			12014.677 ms
		Mudanças de contexto 		1972 voluntário, 175 involuntário

		CONHECIMENTO ADQUIRIDO:

	- concurrent.futures.ThreadPoolExecutor(): Essa abordagem é mais recomendada quando desejamos realizar operações de IO (network) em paralelo, QUANDO a quantidade de requisições é menor.

	- Assincronismo (async): Essa é a melhor abordagem para operações INTENSIVAS de IO (Network) o que NÃO é esse caso, pois também devemos levar em conta o processamento ADICIONAL que é executado explicitamente "por debaixo dos panos" para copntroles internos do python em relação ao async.


-------------------------------------------------------------------------------------------------------------------------------

24o - ScraperAPI: [Teste de performance] Disseminando otimização da performance ENDPOINT DA API: 

VARRENDO COM NENHUM PARAMETRO, GETALL NA HOME: 

Delegando para as API's clones, aonde cada uma responde uma seção `dou` respectiva [MELHOR MÉTRICA PARA ESTE ENDPOINT]:


            Varre tudo da home do portal DOU gov porém NÃO SALVA em banco de dados:
            - GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/



		METRICAS: Delegando a chamada para cada API clone uma seção `dou` respectiva OBTEVE MELHOR RESULTADO PARA ESSE ENDPOINT:
		
		- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/
		
		
		
					CHAMA:
					
					
		- GET http://127.0.0.1:8001/trigger_web_scraping_dou_api/trigger_web_scraping_dou_api/?secao=do1&data=CURRENT
		- GET http://127.0.0.1:8002/trigger_web_scraping_dou_api/trigger_web_scraping_dou_api/?secao=do1&data=CURRENT
		- GET http://127.0.0.1:8003/trigger_web_scraping_dou_api/trigger_web_scraping_dou_api/?secao=do1&data=CURRENT
		
		
		
		Tempo de CPU do usuário 	5014.167 ms
		Tempo de CPU do sistema 	371.048 ms
		Tempo total de CPU 			5385.215 ms
		Tempo decorrido 			7349.486 ms
		Mudanças de contexto 		2412 voluntário, 221 involuntário
		
		
		
		MELHOR METRICA DE ANTES: Utilizando paralelismo com ThreadPoolExecutor() e NÃO DELEGANDO para as API's clones:



		# Executa a função passando como parâmetro cada elemento da lista individualmente, para cada thread diferente:
		# Porisso o `*len(dous_list)` nos outros parâmetros subsequentes que não é a "lista principal" de `dou_list`
		# Por conta deste comportamento aonde a função é executada para cada elemento da lista em paralelo.
		# Logo, os parâmetros devem ser listas com o mesmo tamanho, e como não são listas mesmo igual ao `dou_list`
		# então transformamos esses parâmetros em listas e utilizamos `len()` para que todos os parâmetros da função correspondam com o mesmo tamanho linearmente entre sí.

		```
		    dous_list = ['do1', 'do2', 'do3']
		    all_dous_with_current_date_dontDetails = []
		    with ThreadPoolExecutor() as executor:

		        all_dous_with_current_date_dontDetails = list(executor.map(ScraperUtil.run_scraper_with_all_params, dous_list, [date_now_db_and_brazilian_format]*len(dous_list), [saveInDBFlagURLQueryString]*len(dous_list)))

		    return all_dous_with_current_date_dontDetails ```



		Tempo de CPU do usuário 	4922.568 ms
		Tempo de CPU do sistema 	428.632 ms
		Tempo total de CPU 			5351.200 ms
		Tempo decorrido 			10498.939 ms
		Mudanças de contexto 		4152 voluntário, 188 involuntário


-------------------------------------------------------------------------------------------------------------------------------

25o - ScraperAPI: SUPER Otimizações quase finalizadas e SALVAMENTO dos DETALHADOS: 

	- Realizar requisições nos triggers direto pelo browser é BEM LENTO, isso acontece pois os triggers sempre retornam a massa COMPLETA de uma só vez para o cliente, então a requisição é rápida, porém o carregamento do DOM é prejudicado!
	
	- Realizar requisições para os endpoints que acessam o banco de dados APÓS executar os triggers (gatilhos) é bem mais performatico executa-los no browser, pois as respostas são paginadas, ou seja, não retorna a massa completa de dados, então o carregamento do DOM não é prejudicado!
	
	- Desta  forma, as rotas com `?saveInDBFlag=True` agora são as mais RÁPIDAS, e todas operações tem ela agora!
	

ENDPOINTS DA API:


	SUPER RÁPIDOS: Motivo: Após raspagem, redirecionamento para a rota que consulta o banco local retornando a lista paginada
	
	VARRE TODOS OS DOUS DO DIA; NÃO DETALHA; SALVA NO BANCO:
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=True
	
	
	
	VARRE O DOU MENCIONADO; DETALHA; SALVA NO BANCO:
	
	- GET 127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&detailDOUJournalFlag=True&saveInDBFlag=True
	- GET 127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2&detailDOUJournalFlag=True&saveInDBFlag=True
	- GET 127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3&detailDOUJournalFlag=True&saveInDBFlag=True
	
	
	
	VARRE TODOS OS DOUS DO DIA; DETALHA; SALVA NO BANCO:
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True
	
	
-------------------------------------------------------------------------------------------------------------------------------


26o - [TRADE-OF: PERFORMANCE OU NENHUM ERROR?] Rastreando erros nos endpoints de detalhamentos, observei algo:

	- Utilizar a abordagem anterior de chamadas assíncronas com a lib `aiocfscrape` que adiciona uma camada de async no bypass do cloudflare do `cfscrape` para operações brutas de IO (network) combinando `aiohttp` com `cfscrape` (Porisso o nome `aiocfscrape`), de acordo com essas pesquisas realizadas é a melhor estratégia para web scraping que passa mais tempo em estado de espera dos requests do que fazendo a raspagem em sí do site, TEM UMA OBSERVAÇÃO:
	
	
	
	
	- NOTEI ALGO IMPORTANTE:
	
	
	
	
	- A lib não recebe muitas atualizações a muito tempo, logo, algumas de suas técnicas de bypass para o cloudflare devem não estar funcionando corretamente, pelomenos para os endpoints mais brutos que fazem os detalhamentos dos jornais, pois para outros endpoints funcionou bem. (Percebi que acontece mais com DO3, Talvez por esses dados estarem em servidores mais robustos com firewalls mais modernos e etc.. então enpoints para os detalhamentos dos DO3 é mais protejido).
	
	
	
	- Ocorreu que testando os 3 cenários a seguir, deu para observar alguns pontos:
	
	- DOCUMENTEI: Cada abordagem utilizada foi salvo os resultados completos das raspagens em blocos de notas, para uma melhor visualização do que ocorreu em CADA requisição.
	
	
	
	
	
	- CENÁRIO 1 - Utilizando chamadas assíncronas (`aiocfscrape`) [SAIDAS: ./relatorios/scenario_01_async_with_aiocfscrape.txt]: Foi rápido, porém deu muitos erros nos DO3 (Estratégias de bypass no cloudflare desatualizadas)
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True
	
		```
		@staticmethod
		async def make_request_cloudflare_bypass_async_with_aiocfscrape(url):
			async with CloudflareScraper() as session:
			    async with session.get(url) as resp:
				return await resp.text()``
				
				
			Tempo de CPU do usuário 	8494.555 ms
			Tempo de CPU do sistema 	2276.009 ms
			Tempo total de CPU 		10770.564 ms
			Tempo decorrido 		109216.886 ms
			Mudanças de contexto 		6874 voluntário, 2411 involuntário
	
	
					
				MELHOR ATÉ AGORA: BOA PERFORMANCE E MENOS ERROR: Essa aqui ocorre um erro ou outro, estou rastreando as origens... mas até agora é o que melhor se comportou (em questão de performance e erros)
				
				
	- CENÁRIO 2 - Utilizando multithreading com: [SAIDAS: /scenario_02_multithreading_with_cfscrape.txt]
						
	
	
		```
		@staticmethod
    		async def make_request_cloudflare_bypass_async_multithreading(url):
	
			scraper = cfscrape.create_scraper()
	
			try:
		  	  resp = await asyncio.to_thread(scraper.get, url)
	 	  	 return resp

			except Exception as e:
	 		   print(f"Erro ao fazer a requisição para {url}: {e}")
	 		   return None ```
	 		   
	 
	 
	 - GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True	  
	 
	 
	  
	 		Tempo de CPU do usuário 	15495.015 ms
			Tempo de CPU do sistema 	6047.688 ms
			Tempo total de CPU 		21542.703 ms
			Tempo decorrido 		400719.301 ms
			Mudanças de contexto 		4880 voluntário, 1620 involuntário
	 		  
	
	
	
	
	
	- CENÁRIO 3 - Utilizando apenas neste bloco que faz as requisições utilizando o `cfscrape` normal sem a camada async [SAIDAS: /scenario_03_traditional_prog_with_cfscrape.txt]:
		
		- PIOR performance de todas, já era de se esperar MASSSS foi a única abordagem que RASPOU TUDO SEM ERROR!
		
		PORÉM retronou mais registros do que a quantidade de urlTitle: 
		
			- RESULTADO DO DETALHADO: 6140 LINHAS do arquivo
			- RESULTADO DO NÃO DETALHADO: 3198 Registros no banco.
			
			
			
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True
	
	
			Tempo de CPU do usuário 	38756.132 ms
			Tempo de CPU do sistema 	22963.200 ms
			Tempo total de CPU	 	61719.332 ms
			Tempo decorrido 		1748227.763 ms
			Mudanças de contexto 		8761 voluntário, 2758 involuntário


-------------------------------------------------------------------------------------------------------------------------------

26o - Correção de bugs: Como a tarefa exigia não duplicar códigos similares, eu extraí as verificações da resposta da API em um único bloco, mas isso tornou o código meio acoplado, então vou padronizar as respostas de TODOS endpoints, para evitar problemas de serialização na API MAIN.



ENPOINTS FUNCIONANDO MESMO COM ESSE ACOPLAMENTO (DEVIDO AOS REQUISITOS DA TAREFA DE EVITAR DUPLICAÇÃO DE COD):



- Lembrando que os enpoints que NÃO utiliza a flag `saveInDBFlag=True` são BEM MENOS performáticos, pois a API cospe toda a massa de dados de uma só vez, então isso afeta a renderização do DOM (se estiver executando no browser); 



- Já os enpoints com a flag `saveInDBFlag=True` são mais performáticos, pois após toda a raspagem e salvamento em banco, o cliente é redirecionado para o endpoint que faz a consulta direto na API do banco, e retorna uma ViewSet com páginação, ou seja, não sobrecarrega a renderização do DOM.

	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=True

		docker exec -it psql psql -U CHANGE-ME -d db_poder360
		DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&saveInDBFlag=True

		DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou;

		OU NÃO PRECISA APAGAR POIS NÃO ESTA INSERINDO DUPLICATAS! 

	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=18-01-2024&saveInDBFlag=True




			ENPOINTS COM ALGUMAS OBSERVAÇÕES DITAS NO ULTIMO COMMIT: (Podem apresentar problemas de serialização, por conta do acoplamento nas respostas.. mas em breve estara 100% funcional ;D)

	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True

	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&detailDOUJournalFlag=True&saveInDBFlag=True


	OBS: ESSAS ROTAS DE DETALHAMENTO PARA AS 3 SEÇÕES DOS DOUS: Esse algoritmo vai ser melhorado, pois geralmente a seção `do3` tem mais dados, logo, as duas outras instâncias clones da API terminam primeiro e ficam ocilando. A ideia é: Fazer cada API processar 1/3 dos dados e enviar os outros 2/3 para serem divididos entre as outras duas instâncias da API, desta forma podemos GARANTIR que todas as APIS vão processar a mesma quantidade de dados.  
	
	
-------------------------------------------------------------------------------------------------------------------------------

27o - Correção de bugs nos endpoints de detalhamentos finalizado:

	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&detailDOUJournalFlag=True&saveInDBFlag=True
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2&detailDOUJournalFlag=True&saveInDBFlag=True
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3&detailDOUJournalFlag=True&saveInDBFlag=True



Toda vez que executado o salvamento dos jornais com detalhamentos, ocorriam erros na inserção:


	- Erro invisivel, pois não exibia nenhum exceção, e esse problema estava na seguinte linha do model `DetailSingleJournalOfDOU`:
		```
		DetailSingleJournalOfDOU.objects.bulk_create([DetailSingleJournalOfDOU(**item) for item in not_exists_records_list], ignore_conflicts=True)```    
		
	
	- O erro ocorria pois a ORM do django utiliza a coluna `id` para evitar duplicatas na inserção, porém como não existe esse campo no html de raspagem, eu tive que definir outro atributo do objeto para utilizar nesta verificação.
	
	- O atributo escolhido foi o `versao_certificada`, pois eu ACHAVA que esse atributo era UNICO para cada registro, PORÉM não é, pois essa versão certificada aponta para o jornal, e nesta página do jornal possuem OUTROS registros, ou seja, não se tratava de APENAS um dos atos, mas sim de vários outros, conforme mostrado a seguir:
	
	
	
		db_poder360=# SELECT title FROM trigger_web_scraping_dou_api_detailsinglejournalofdou WHERE "versao_certificada" = 'http://pesquisa.in.gov.br/imprensa/jsp/visualiza/index.jsp?data=19/01/2024&jornal=529&pagina=1';
		
		
		
		
					    title                    
			---------------------------------------------
			 PORTARIA Nº 1.552, DE 16 DE JANEIRO DE 2024
			 PORTARIA Nº 1.547, DE 16 DE JANEIRO DE 2024
			 PORTARIA Nº 6, DE 18 DE JANEIRO DE 2024
			 PORTARIA Nº 1.555, DE 16 DE JANEIRO DE 2024
			 PORTARIAS DE 18 DE JANEIRO DE 2024
			 PORTARIA Nº 1.551, DE 16 DE JANEIRO DE 2024
			 
			(6 rows)
			
	
	- SOLUÇÃO: Utilizar TODOS os atributos do objeto para garantir DISTINCT.
	
	- OBS: Como esse erro não estava visivel no outro model para os registros dos jornais não detalhados ``
	
	
	
	- MESMO ASSIM EU REFORÇEI CONFIGURANDO A ORM PARA UTILIZAR O ATRIBUTO `urlTitle` NAS VERIFICAÇÕES DE DUPLICATAS:
	
			class JournalJsonArrayOfDOU(models.Model):
			    id = models.AutoField(primary_key=True)
			    pubName = models.CharField(null=True)
			    urlTitle = models.CharField(unique=True)        <----- BASTA DEFINIR ESSA CONSTRAINT
			    numberPage = models.IntegerField(null=True)
			    subTitulo = models.TextField(null=True)
			    titulo = models.TextField(null=True)
			    title = models.TextField(null=True)
			    pubDate = models.DateField(null=True)
			    content = models.TextField(null=True)
			    editionNumber = models.IntegerField(null=True)
			    hierarchyLevelSize = models.IntegerField(null=True)
			    artType = models.CharField(null=True)
			    pubOrder = models.CharField(null=True)
			    hierarchyStr = models.TextField(null=True)
			    hierarchyList = models.JSONField(null=True)
	
	
		POIS `urlTitle` é um atributo que o valor sempre é diferente entre os registros (Conforme visto na query a seguir), logo, podemos utilizar ele para garantir DISTINCT. (Aparentemente os resultados são os mesmos que os anteriores (sem definir essa constraint), mas resolvi reforçar essa validação.
		
		db_poder360=# SELECT content from trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "urlTitle" = 'portaria-n-9-de-18-de-janeiro-de-2024-538
		
		
							content
			---------------------------------------------------------------------------                                                                                                                                                                                                                                 
			
				 PORTARIA Nº 9, DE 18 DE JANEIRO DE 2024 O(A) SECRETÁRIA DO AUDIOVISUAL, no uso de suas atribuições legais, que lhe confere a Portaria nº 1.408, de 31 de janeiro de 2023 e o art. 1º da Portaria nº 1.201, de 18 de dezembro de 2009, resolve: Art. 1.º - Aprovar o(s) projeto(s) cultural(is), relacionado(s) no(s) anexo(s) desta Portaria, para o(s) qual(is) o(s) proponente(s) fica(m) autorizado(s) a capt...
				 
				(1 row)   <--- APENAS 1 Linha afetada em 3182 Registros (19-01-2024)
				
				
				

	ATENÇÃO, SEMPRE RESETE O CONTAINER PARA GARANTIR ATUALIZAÇÃO CORRETA:
	
		 $ sudo rm -r ./data/  
	         $ docker stop $(sudo docker ps -a -q) ; sudo docker system prune -f ; sudo docker rm -vf $(sudo docker ps -aq) ; sudo docker rmi -f $(sudo docker images -aq) ;   
	
	

	
	PARA FACILITAR A VERIFICAÇÃO DA INTEGRIDADE DOS RESULTADOS:
	
	
	docker exec -it psql psql -U CHANGE-ME -d db_poder360 <---- Executar comandos no container do postegresql pelo bash
	
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou; <---- Apaga os registros da tabela dos DOU's NÃO DETALHADOS.
	
	
	DELETE FROM trigger_web_scraping_dou_api_journaljsonarrayofdou WHERE "pubName" = 'DO1'; <--- Apaga apenas os registros do DO1 - NÃO DETALHADO.
	
	
	- GET 127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&saveInDBFlag=True <--- Faz a consulta do DO1 NÃO DETALHADO para verificar integridade (Pode executar várias vezes que NÃO VAI DUPLICAR os elementos).
	
	
	
	
	DELETE FROM trigger_web_scraping_dou_api_detailsinglejournalofdou; <---- Apaga os registros da tabela dos DOU's DETALHADOS.
	
	
	
	ENDPOINTS: OBS SEMPRE UTILIZAR `&saveInDBFlag=True` nos parâmetros pois esses são os endpoints mais rápidos, motivos informados nos ultimos commits! ;D
	
	
	NÃO DETALHADO (NOVOS E MAIS RÁPIDOS + ANTIGOS DO ULTIMO E-MAIL): 
	
	
	

	VARRENDO COM NENHUM PARAMETRO, TODOS DO1, DO2 e DO3 DO DIA
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?saveInDBFlag=True
	
	
	
	
	VARRENDO COM SEÇÃO (MAIS RÁPIDO QUE OUTRO): 
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&saveInDBFlag=True
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2&saveInDBFlag=True
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3&saveInDBFlag=True
	
	
	
	
	VARRENDO COM DATA (MAIS RÁPIDO QUE OUTRO): 
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=12-01-2024&saveInDBFlag=true
	
	
	
	
	VARRENDO COM DATA E SEÇÃO (MAIS RÁPIDO QUE OUTRO): 
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&data=12-01-2024&saveInDBFlag=True
	
	
	
	- ENPOINT QUE FAZ A REQUISIÇÃO PARA O BANCO DOS REGISTROS NÃO DETALHADOS: http://127.0.0.1:8000/db_dou_api/journaljsonarrayofdouviewset/
	
	
	
	
	
	DETALHADOS: Obs: Mesmo aplicando lógicas de retentaivas quando falha a requisição no servidor do gov, as vezes menos de 10 elementos vem com valores nullos, mas basta fazer a mesma requisição novamente que pode conseguir + elementos, fique tranquilo pois o sistema não insere duplicatas ou valores NULL!! Vou implementar uma lógica de retentativa mais robusta. Mas já esta usável rsrs.. ;D 
	
	VARRENDO COM NENHUM PARAMETRO, TODOS DO1, DO2 e DO3 DO DIA (MAIS RÁPIDO QUE OUTRO)
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True
	
	
	
	
	VARRENDO COM SEÇÃO (MAIS RÁPIDO QUE OUTRO): 
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&detailDOUJournalFlag=True&saveInDBFlag=True
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do2&detailDOUJournalFlag=True&saveInDBFlag=True
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do3&detailDOUJournalFlag=True&saveInDBFlag=True
	
	
	
	
	VARRENDO COM DATA (MAIS RÁPIDO QUE OUTRO): 
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?data=12-01-2024&detailDOUJournalFlag=True&saveInDBFlag=true
	
	
	
	
	VARRENDO COM DATA E SEÇÃO (MAIS RÁPIDO QUE OUTRO): 
	
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?secao=do1&data=12-01-2024&detailDOUJournalFlag=True&saveInDBFlag=True
	
	
	
	- ENPOINT QUE FAZ A REQUISIÇÃO PARA O BANCO DOS REGISTROS DETALHADOS: http://127.0.0.1:8000/db_dou_api/detailsinglejournalofdouviewset/
	
            
            
#  OBS: CASO OCORRER O SEGUINTE ERROR:
``
djangoapp              | Internal Server Error: /trigger_web_scraping_dou_api/
djangoapp              | Traceback (most recent call last):
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/connection.py", line 174, in _new_conn
djangoapp              |     conn = connection.create_connection(
djangoapp              |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/util/connection.py", line 73, in create_connection
djangoapp              |     for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
djangoapp              |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/usr/local/lib/python3.11/socket.py", line 962, in getaddrinfo
djangoapp              |     for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
djangoapp              |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              | socket.gaierror: [Errno -3] Try again
djangoapp              | 
djangoapp              | During handling of the above exception, another exception occurred:
djangoapp              | 
djangoapp              | Traceback (most recent call last):
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/connectionpool.py", line 699, in urlopen
djangoapp              |     httplib_response = self._make_request(
djangoapp              |                        ^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/connectionpool.py", line 394, in _make_request
djangoapp              |     conn.request(method, url, **httplib_request_kw)
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/connection.py", line 239, in request
djangoapp              |     super(HTTPConnection, self).request(method, url, body=body, headers=headers)
djangoapp              |   File "/usr/local/lib/python3.11/http/client.py", line 1283, in request
djangoapp              |     self._send_request(method, url, body, headers, encode_chunked)
djangoapp              |   File "/usr/local/lib/python3.11/http/client.py", line 1329, in _send_request
djangoapp              |     self.endheaders(body, encode_chunked=encode_chunked)
djangoapp              |   File "/usr/local/lib/python3.11/http/client.py", line 1278, in endheaders
djangoapp              |     self._send_output(message_body, encode_chunked=encode_chunked)
djangoapp              |   File "/usr/local/lib/python3.11/http/client.py", line 1038, in _send_output
djangoapp              |     self.send(msg)
djangoapp              |   File "/usr/local/lib/python3.11/http/client.py", line 976, in send
djangoapp              |     self.connect()
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/connection.py", line 205, in connect
djangoapp              |     conn = self._new_conn()
djangoapp              |            ^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/connection.py", line 186, in _new_conn
djangoapp              |     raise NewConnectionError(
djangoapp              | urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f89d26166d0>: Failed to establish a new connection: [Errno -3] Try again
djangoapp              | 
djangoapp              | During handling of the above exception, another exception occurred:
djangoapp              | 
djangoapp              | Traceback (most recent call last):
djangoapp              |   File "/venv/lib/python3.11/site-packages/requests/adapters.py", line 486, in send
djangoapp              |     resp = conn.urlopen(
djangoapp              |            ^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/connectionpool.py", line 755, in urlopen
djangoapp              |     retries = retries.increment(
djangoapp              |               ^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/urllib3/util/retry.py", line 574, in increment
djangoapp              |     raise MaxRetryError(_pool, url, error or ResponseError(cause))
djangoapp              | urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='djangoappclonetwo', port=8002): Max retries exceeded with url: /trigger_web_scraping_dou_api/?secao=do2&data=19-01-2024 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f89d26166d0>: Failed to establish a new connection: [Errno -3] Try again'))
djangoapp              | 
djangoapp              | During handling of the above exception, another exception occurred:
djangoapp              | 
djangoapp              | Traceback (most recent call last):
djangoapp              |   File "/venv/lib/python3.11/site-packages/django/core/handlers/exception.py", line 55, in inner
djangoapp              |     response = get_response(request)
djangoapp              |                ^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/django/core/handlers/base.py", line 197, in _get_response
djangoapp              |     response = wrapped_callback(request, *callback_args, **callback_kwargs)
djangoapp              |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/django/views/decorators/csrf.py", line 56, in wrapper_view
djangoapp              |     return view_func(*args, **kwargs)
djangoapp              |            ^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/django/views/generic/base.py", line 104, in view
djangoapp              |     return self.dispatch(request, *args, **kwargs)
djangoapp              |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/rest_framework/views.py", line 509, in dispatch
djangoapp              |     response = self.handle_exception(exc)
djangoapp              |                ^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/rest_framework/views.py", line 469, in handle_exception
djangoapp              |     self.raise_uncaught_exception(exc)
djangoapp              |   File "/venv/lib/python3.11/site-packages/rest_framework/views.py", line 480, in raise_uncaught_exception
djangoapp              |     raise exc
djangoapp              |   File "/venv/lib/python3.11/site-packages/rest_framework/views.py", line 506, in dispatch
djangoapp              |     response = handler(request, *args, **kwargs)
djangoapp              |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/djangoapp/trigger_web_scraping_dou_api/views.py", line 58, in get
djangoapp              |     return self.handle_URL_empty_params(saveInDBFlag, detailDOUJournalFlag)
djangoapp              |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/djangoapp/trigger_web_scraping_dou_api/views.py", line 192, in handle_URL_empty_params
djangoapp              |     response = ScraperUtil.run_scraper_with_empty_params_using_clone_instances(detailDOUJournalFlag)
djangoapp              |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/djangoapp/trigger_web_scraping_dou_api/scrapers.py", line 294, in run_scraper_with_empty_params_using_clone_instances
djangoapp              |     return ScraperUtil.run_scraper_using_clone_instances_when_secao_do1_do2_and_do3(date_now_db_and_brazilian_format,
djangoapp              |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/djangoapp/trigger_web_scraping_dou_api/scrapers.py", line 336, in run_scraper_using_clone_instances_when_secao_do1_do2_and_do3
djangoapp              |     all_dous_response = list(executor.map(ScraperUtil.make_request_for_others_clone_api,
djangoapp              |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/usr/local/lib/python3.11/concurrent/futures/_base.py", line 619, in result_iterator
djangoapp              |     yield _result_or_cancel(fs.pop())
djangoapp              |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/usr/local/lib/python3.11/concurrent/futures/_base.py", line 317, in _result_or_cancel
djangoapp              |     return fut.result(timeout)
djangoapp              |            ^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/usr/local/lib/python3.11/concurrent/futures/_base.py", line 456, in result
djangoapp              |     return self.__get_result()
djangoapp              |            ^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/usr/local/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
djangoapp              |     raise self._exception
djangoapp              |   File "/usr/local/lib/python3.11/concurrent/futures/thread.py", line 58, in run
djangoapp              |     result = self.fn(*self.args, **self.kwargs)
djangoapp              |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/djangoapp/trigger_web_scraping_dou_api/scrapers.py", line 346, in make_request_for_others_clone_api
djangoapp              |     response = requests.get(url)
djangoapp              |                ^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/requests/api.py", line 73, in get
djangoapp              |     return request("get", url, params=params, **kwargs)
djangoapp              |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/requests/api.py", line 59, in request
djangoapp              |     return session.request(method=method, url=url, **kwargs)
djangoapp              |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/requests/sessions.py", line 589, in request
djangoapp              |     resp = self.send(prep, **send_kwargs)
djangoapp              |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/requests/sessions.py", line 703, in send
djangoapp              |     r = adapter.send(request, **kwargs)
djangoapp              |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
djangoapp              |   File "/venv/lib/python3.11/site-packages/requests/adapters.py", line 519, in send
djangoapp              |     raise ConnectionError(e, request=request)
djangoapp              | requests.exceptions.ConnectionError: HTTPConnectionPool(host='djangoappclonetwo', port=8002): Max retries exceeded with url: /trigger_web_scraping_dou_api/?secao=do2&data=19-01-2024 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f89d26166d0>: Failed to establish a new connection: [Errno -3] Try again'))
djangoapp              | [19/Jan/2024 23:48:42] "GET /trigger_web_scraping_dou_api/?saveInDBFlag=1 HTTP/1.1" 500 247753
djangoapp              | /djangoapp/trigger_web_scraping_dou_api/scrapers.py changed, reloading.
^CGracefully stopping... (press Ctrl+C again to force)
``


## BASTA RE-BUILDAR O DOCKER E REINICIAR TUDO, ESTOU RASTREANDO ESSE ERRO! ;D 

### Quando faz isso funciona denovo:

```bash
$ cd ./web
$ sudo rm -r ./data/
$ docker stop $(sudo docker ps -a -q) ; sudo docker system prune -f ; sudo docker rm -vf $(sudo docker ps -aq) ; sudo docker rmi -f $(sudo docker images -aq)
$ docker-compose up --build --force-recreate
```	


-------------------------------------------------------------------------------------------------------------------------------

28o - ScraperAPI: Balançeando a carga igualmente entre as instâncias clones da API:

- Parece que não obteve ganhos, com muitas requisições seguindas ocorrem mais erros e desta forma mais retentativas são necessárias para opter respostas, então o tempo ganho é perdido nessas retentativas.

- Talvez se melhorar a lógica do retry, para aguardar um tempo, ou algo similar, pode contornar esse problema!


- O código de antes se manteve, eu apenas adicionei mais uma flag:




	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True&balancerFlag=True
	
	
	
	Tempo de CPU do usuário 	81.962 ms
	Tempo de CPU do sistema 	7.605 ms
	Tempo total de CPU 		89.567 ms
	Tempo decorrido 		107.535 ms
	Mudanças de contexto 		13 voluntário, 9 involuntário
	


- Fiz isso para testar mais facilmente, a versão anterior (sem balançear a carga igualmente entre todas instâncias clones da API: (Mas ainda utilizando elas, aonde cada instância é responsável por uma seção DOU.




	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True




	Tempo de CPU do usuário 	108.957 ms
	Tempo de CPU do sistema 	2.403 ms
	Tempo total de CPU 		111.360 ms
	Tempo decorrido 		125.128 ms
	Mudanças de contexto 		12 voluntário, 13 involuntário
	
	
-------------------------------------------------------------------------------------------------------------------------------

29o - ScraperAPI: Super Otimização na performance com `curl_cffi` fase 01: Substituí o `cfscrape` pelo `curl_cffi` que possuí funções async por natureza e também realiza bypass nas restrições do cloudflare. 

- ALÉM de `curl_cffi` possuir funções async, ele é implementado em C, logo, a performance é gritante.

- Por conta da maneira que devemos implementar, não funcionou bem com o `retry` do tenacity, mas fiz uma solução de retentativas adicionando na lista de `urls_fails` para urls que status_code != 200.

- Também por conta do ponto acima, não é possivel realizar prints durante as requisições, apenas quando todas finaliza.

- Primeiro realizamos as requisições em massa, e depois que tudo esta ok, realizamos o scraping na lista de sites carregados (htmls adicionados na lista).

- Apesar de ser bem rápido, ocorrem bastante problemas nas restrições do cloudflare para a seção do3, então o código passa bastante tempo na lógica de retentativa mencionada anteriormente.

- Estou em busca de uma solução mais viável para esse bypass, porém mesmo assim já está mais rápido que a solução anterior utilizando o `cfscrape` com uma camada async adicionada com `asyncio.to_thread(scraper.get, url, timeout=10)`.

- O `curl_cffi` tem forte acoplamentos com o `curl_impersonate` e ele não estava disponivel no repositório do alpine, então tive que utilizar o mesmo Dockerfile do projeto no github do `curl_impersonate`, e realizar pequenas adaptações para o nosso projeto django: https://github.com/lwthiker/curl-impersonate/tree/main/chrome

		
		
	- GET http://127.0.0.1:8000/trigger_web_scraping_dou_api/?detailDOUJournalFlag=True&saveInDBFlag=True
	
	- Data: 23/01/2024



		METRICAS ANTES (Utilizando `cfscrape` com uma camada async adicionada nele com `resp = await asyncio.to_thread(scraper.get, url, timeout=10)`):
		
		
		
			Tempo de CPU do usuário 	121.322 ms
			Tempo de CPU do sistema 	48.392 ms
			Tempo total de CPU 		169.714 ms
			Tempo decorrido 		261.900 ms
			Mudanças de contexto 		24 voluntário, 8 involuntário
		
		
		
		METRICAS DEPOIS (COM O `curl_cffi` feito com C, async por natureza e com bypass do cloudflare):
		
		
		
			Tempo de CPU do usuário 	53.927 ms
			Tempo de CPU do sistema 	17.324 ms
			Tempo total de CPU 		71.251 ms
			Tempo decorrido 		88.731 ms
			Mudanças de contexto 		16 voluntário, 20 involuntário
			


## LEMBRE-SE DE RE-BUILDAR O DOCKER E REINICIAR TUDO PARA ASSUMIR A NOVA IMAGEM:
		
		
		
	```bash
	$ cd ./web
	$ sudo rm -r ./data/
	$ docker stop $(sudo docker ps -a -q) ; sudo docker system prune -f ; sudo docker rm -vf $(sudo docker ps -aq) ; sudo docker rmi -f $(sudo docker images -aq)
	$ docker-compose up --build --force-recreate
	```		



## ATENÇÃO IMPORTANTE: Caso ocorra o seguinte erro na hora de buildar a imagem nova do Docker:



	Err:19 http://deb.debian.org/debian bookworm/main amd64 python3.11 amd64 3.11.2-6
	  Could not resolve 'deb.debian.org'
	Err:20 http://deb.debian.org/debian bookworm/main amd64 libpython3-stdlib amd64 3.11.2-1+b1
	  Could not resolve 'deb.debian.org'
	Err:21 http://deb.debian.org/debian bookworm/main amd64 python3 amd64 3.11.2-1+b1
	  Could not resolve 'deb.debian.org'
	Err:22 http://deb.debian.org/debian bookworm/main amd64 krb5-locales all 1.20.1-2+deb12u1
	  Could not resolve 'deb.debian.org'
	Err:23 http://deb.debian.org/debian bookworm/main amd64 libgpm2 amd64 1.20.7-10+b1
	  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/p/python3.11/libpython3.11-minimal_3.11.2-6_amd64.deb  Temporary failure resolving 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/e/expat/libexpat1_2.5.0-1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/p/python3.11/python3.11-minimal_3.11.2-6_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/p/python3-defaults/python3-minimal_3.11.2-1%2bb1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/m/media-types/media-types_10.0.0_all.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/n/ncurses/libncursesw6_6.4-4_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/k/krb5/libkrb5support0_1.20.1-2%2bdeb12u1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/k/krb5/libk5crypto3_1.20.1-2%2bdeb12u1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/k/keyutils/libkeyutils1_1.6.3-2_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/k/krb5/libkrb5-3_1.20.1-2%2bdeb12u1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/k/krb5/libgssapi-krb5-2_1.20.1-2%2bdeb12u1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/libt/libtirpc/libtirpc-common_1.3.3%2bds-1_all.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/libt/libtirpc/libtirpc3_1.3.3%2bds-1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/libn/libnsl/libnsl2_1.3.0-2_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/r/readline/readline-common_8.2-1.3_all.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/r/readline/libreadline8_8.2-1.3_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/s/sqlite3/libsqlite3-0_3.40.1-2_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/p/python3.11/libpython3.11-stdlib_3.11.2-6_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/p/python3.11/python3.11_3.11.2-6_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/p/python3-defaults/libpython3-stdlib_3.11.2-1%2bb1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/p/python3-defaults/python3_3.11.2-1%2bb1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/k/krb5/krb5-locales_1.20.1-2%2bdeb12u1_all.deb  Could not resolve 'deb.debian.org'
	E: Failed to fetch http://deb.debian.org/debian/pool/main/g/gpm/libgpm2_1.20.7-10%2bb1_amd64.deb  Could not resolve 'deb.debian.org'
	E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
	The command '/bin/sh -c apt-get update && apt-get install -y ca-certificates &&     apt-get install -y python3 && apt-get install -y python3.11-venv && apt-get install -y ncat     && rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100
	ERROR: Service 'djangoappclonethree' failed to build : Build failed



## A SOLUÇÃO É BUILDAR NOVAMENTE COM: `docker-compose up --build`
